"""
–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏
–ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –ø—Ä–æ–≤–µ–¥–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏
–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏.

–†–∞–∑–¥–µ–ª–∏–º –≤—ã–±–æ—Ä–∫—É –Ω–∞ –æ–±—É—á–∞—é—â—É—é/–ø—Ä–æ–≤–µ—Ä–æ—á–Ω—É—é –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 80/20.

–ü—Ä–∏–º–µ–Ω–∏–º –Ω–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∫–æ—Ä–∏–Ω–≥–∞. –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ
–≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã.

–ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∫–∞–ø–ø–∞-–º–µ—Ç—Ä–∏–∫—É –∏ –º–∞—Ç—Ä–∏—Ü—É –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π.

–î–∞–Ω–Ω—ã–µ:

https://video.ittensive.com/machine-learning/prudential/train.csv.gz

–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ: https://www.kaggle.com/c/prudential-life-insurance-assessment/

¬© ITtensive, 2020
"""


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn import preprocessing


pd.set_option('display.max_rows', None)  # –°–±—Ä–æ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
# –≤—ã–≤–æ–¥–∏–º—ã—Ö —Ä—è–¥–æ–≤
pd.set_option('display.max_columns', None)  # –°–±—Ä–æ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ —á–∏—Å–ª–æ
# –≤—ã–≤–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
pd.set_option('display.max_colwidth', None)  # –°–±—Ä–æ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
# —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∑–∞–ø–∏—Å–∏


"""–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""

data = pd.read_csv(
    "https://video.ittensive.com/machine-learning/prudential/train.csv.gz"
)
print(data.info())


"""–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""

data["Product_Info_2_1"] = data["Product_Info_2"].str.slice(0, 1)
data["Product_Info_2_2"] = pd.to_numeric(data["Product_Info_2"].str.slice(1, 2))
data.drop(labels=["Product_Info_2"], axis=1, inplace=True)
print(data.info())


"""–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""


def reduce_mem_usage (df):
    start_mem = df.memory_usage().sum() / 1024 ** 2
    for col in df.columns:
        col_type = df[col].dtypes
        if str(col_type)[:5] == "float":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.finfo("f2").min and c_max < np.finfo("f2").max:
                df[col] = df[col].astype(np.float16)
            elif c_min > np.finfo("f4").min and c_max < np.finfo("f4").max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col].astype(np.float64)
        elif str(col_type)[:3] == "int":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.iinfo("i1").min and c_max < np.iinfo("i1").max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo("i2").min and c_max < np.iinfo("i2").max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo("i4").min and c_max < np.iinfo("i4").max:
                df[col] = df[col].astype(np.int32)
            elif c_min > np.iinfo("i8").min and c_max < np.iinfo("i8").max:
                df[col] = df[col].astype(np.int64)
        else:
            df[col] = df[col].astype("category")
    end_mem = df.memory_usage().sum() / 1024 ** 2
    print("–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–µ–Ω—å—à–µ –Ω–∞", round(start_mem - end_mem, 2),
          "–ú–± (–º–∏–Ω—É—Å)", round(100 * (start_mem - end_mem) / start_mem, 1), "%)")
    return df


data = reduce_mem_usage(data)
print(data.info())


"""–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è, –µ–¥–∏–Ω–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã"""

#
# Product
#    A
#    B
#    C
#    A
#
# –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –≤
#
# ProductA  ProductB  ProductC
#     1         0         0
#     0         1         0
#     0         0         1
#     1         0         0

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sklearn.preprocessing.OneHotEncoder, –Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ
# –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ñ—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–±–æ—Ä –µ–¥–∏–Ω–∏—á–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
# –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ—Ä—Ç–µ–∂–∞ –¥–∞–Ω–Ω—ã—Ö).
#
# –¢–∞–∫–∂–µ –Ω–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
# (A->1, B->2, C->3, D->4, E->5), –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –Ω–æ–º–∏–Ω–∞—Ç–∏–≤–Ω—É—é
# —Å–ª—É—á–∞–π–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É –≤ —Ä–∞–Ω–≥–æ–≤—É—é/—á–∏—Å–ª–æ–≤—É—é, –∏ —è–≤–ª—è–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –¥–æ–ø—É—â–µ–Ω–∏–µ–º
# –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

for l in data["Product_Info_2_1"].unique():
    data["Product_Info_2_1" + l] = data["Product_Info_2_1"].isin([l]).astype("int8")
data.drop(labels=["Product_Info_2_1"], axis=1, inplace=True)


"""–ó–∞–ø–æ–ª–Ω–∏–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è"""

# -1 —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç "—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ" –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π

data.fillna(value=-1, inplace=True)


"""–°—Ç–æ–ª–±—Ü—ã –¥–ª—è –º–æ–¥–µ–ª–∏"""

columns_groups = [
    "Insurance_History",
    "InsuredInfo",
    "Medical_Keyword",
    "Family_Hist",
    "Medical_History",
    "Product_Info"
]
columns = ["Wt", "Ht", "Ins_Age", "BMI"]
for cg in columns_groups:
    columns.extend(data.columns[data.columns.str.startswith(cg)])
print(columns)


"""–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ–¥–µ–º z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
# (preprocessing). –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å—å –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.

scaler = preprocessing.StandardScaler()
scaler.fit(pd.DataFrame(data, columns=columns))


"""–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""

data_train, data_test = train_test_split(data, test_size=0.2)
print(data_train.head())


"""–†–∞—Å—á–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞–∏–≤–Ω–æ–≥–æ –ë–∞–π–µ—Å–∞"""

# ùëÉ(ùê¥‚à£ùêµ)=(ùëÉ(ùêµ‚à£ùê¥)*ùëÉ(ùê¥))/(ùëÉ(ùêµ))
#
# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω—è—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ
# –∑–Ω–∞—á–µ–Ω–∏–µ - P(B). –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
# (–ø–æ —Ñ–∞–∫—Ç—É, –¥–æ–ª—è) - P(A). –ó–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
# –ø—Ä–∏–Ω—è—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ - P(B\A).
#
# –ü–æ –≤—Å–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
# –ø—Ä–∏–Ω—è—Ç—å –∫–∞–∫–æ–µ-–ª–∏–±–æ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞.

y = data_train["Response"]
x = scaler.transform(pd.DataFrame(data_train, columns=columns))

bayes = GaussianNB()
bayes.fit(x, y)


"""–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""

data_test = pd.DataFrame(data_test)
x_test = scaler.transform(pd.DataFrame(data_test, columns=columns))
data_test["target"] = bayes.predict(x_test)
print(data_test.head())


"""–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""

print("–ë–∞–π–µ—Å:", cohen_kappa_score(
    data_test["target"],
    data_test["Response"],
    weights="quadratic"
))


"""–ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π"""

print(confusion_matrix(data_test["target"], data_test["Response"]))
