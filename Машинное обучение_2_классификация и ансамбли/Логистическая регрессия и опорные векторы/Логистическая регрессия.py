"""
ÐŸÐ¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð·Ð°Ð´Ð°Ñ‡Ð¸
Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ¼ Ð¸Ñ… Ðº Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ð¼, Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸Ð¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¸, Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ð¼ÑÑ‚ÑŒ.

Ð Ð°Ð·Ð´ÐµÐ»Ð¸Ð¼ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÑƒ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ/Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¾Ñ‡Ð½ÑƒÑŽ Ð² ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¸ 80/20.

ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼ Ð»Ð¾Ð³Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸ÑŽ Ð¿Ð¾ Ð²ÑÐµÐ¼Ñƒ Ð½Ð°Ð±Ð¾Ñ€Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ….

ÐŸÑ€Ð¾Ð²ÐµÐ´ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ð¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡ÐµÑ€ÐµÐ· ÐºÐ°Ð¿Ð¿Ð°-Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ.

Ð”Ð°Ð½Ð½Ñ‹Ðµ:

https://video.ittensive.com/machine-learning/prudential/train.csv.gz

Ð¡Ð¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ðµ: https://www.kaggle.com/c/prudential-life-insurance-assessment/

Â© ITtensive, 2020
"""


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing


pd.set_option('display.max_rows', None)  # Ð¡Ð±Ñ€Ð¾Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ Ð½Ð° ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
# Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð²
pd.set_option('display.max_columns', None)  # Ð¡Ð±Ñ€Ð¾Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ Ð½Ð° Ñ‡Ð¸ÑÐ»Ð¾
# Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð²
pd.set_option('display.max_colwidth', None)  # Ð¡Ð±Ñ€Ð¾Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ Ð½Ð° ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
# ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð² Ð·Ð°Ð¿Ð¸ÑÐ¸


"""Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""

data = pd.read_csv(
    "https://video.ittensive.com/machine-learning/prudential/train.csv.gz"
)
print(data.info())


"""ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""

data["Product_Info_2_1"] = data["Product_Info_2"].str.slice(0, 1)
data["Product_Info_2_2"] = pd.to_numeric(data["Product_Info_2"].str.slice(1, 2))
data.drop(labels=["Product_Info_2"], axis=1, inplace=True)
for l in data["Product_Info_2_1"].unique():
    data["Product_Info_2_1" + l] = data["Product_Info_2_1"].isin([l]).astype("int8")
data.drop(labels=["Product_Info_2_1"], axis=1, inplace=True)
data.fillna(value=-1, inplace=True)


"""ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸"""


def reduce_mem_usage (df):
    start_mem = df.memory_usage().sum() / 1024 ** 2
    for col in df.columns:
        col_type = df[col].dtypes
        if str(col_type)[:5] == "float":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.finfo("f2").min and c_max < np.finfo("f2").max:
                df[col] = df[col].astype(np.float16)
            elif c_min > np.finfo("f4").min and c_max < np.finfo("f4").max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col].astype(np.float64)
        elif str(col_type)[:3] == "int":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.iinfo("i1").min and c_max < np.iinfo("i1").max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo("i2").min and c_max < np.iinfo("i2").max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo("i4").min and c_max < np.iinfo("i4").max:
                df[col] = df[col].astype(np.int32)
            elif c_min > np.iinfo("i8").min and c_max < np.iinfo("i8").max:
                df[col] = df[col].astype(np.int64)
        else:
            df[col] = df[col].astype("category")
    end_mem = df.memory_usage().sum() / 1024 ** 2
    print('ÐŸÐ¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð½Ð°', round(start_mem - end_mem, 2),
          'ÐœÐ± (Ð¼Ð¸Ð½ÑƒÑ', round(100 * (start_mem - end_mem) / start_mem, 1), '%)')
    return df


data = reduce_mem_usage(data)
print(data.info())


"""ÐÐ°Ð±Ð¾Ñ€ ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð² Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð°"""

columns_groups = [
    "Insurance_History",
    "InsurÐµdInfo",
    "Medical_Keyword",
    "Family_Hist",
    "Medical_History",
    "Product_Info"
]
columns = ["Wt", "Ht", "Ins_Age", "BMI"]
for cg in columns_groups:
    columns.extend(data.columns[data.columns.str.startswith(cg)])
print(columns)


"""ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""

# Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ¼ z-Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ
# Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ (preprocessing).

scaler = preprocessing.StandardScaler()
scaler.fit(pd.DataFrame(data, columns=columns))


"""Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""

# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð½Ð°Ð±Ð¾Ñ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…

data_train, data_test = train_test_split(data, test_size=0.2)
data_train = pd.DataFrame(data_train)
data_test = pd.DataFrame(data_test)
print(data_train.head())


"""Ð›Ð¾Ð³Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ"""

# ð‘ƒ=ð‘’ð‘¥ð‘**ð‘‡/(1+ð‘’ð‘¥ð‘**ð‘‡)
# ð‘‡=ð‘Ž0+ð‘1ð‘¥1+â‹¯+ð‘ð‘›ð‘¥ð‘›
# T - Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ñ‚Ð¾Ñ€, Ð»Ð¾Ð³Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÑ€Ð¸Ð²Ð°Ñ


def regression_model(df, columns):
    y = df["Response"]
    x = scaler.transform(pd.DataFrame(df, columns=columns))
    model = LogisticRegression(
        max_iter=1000,
        class_weight='balanced',
        multi_class='multinomial'
    )
    model.fit(x, y)
    return model


def logistic_regression(columns):
    x = scaler.transform(pd.DataFrame(data_test, columns=columns))
    model = regression_model(data_train, columns)
    data_test["target"] = model.predict(x)
    return cohen_kappa_score(
        data_test["target"],
        data_test["Response"],
        weights="quadratic"
    )


"""ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸"""

# ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð°ÐµÑ‚ 0.192, kNN(100) - 0.3

print("Ð›Ð¾Ð³Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ:", round(logistic_regression(columns), 3))

# Ð’ ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð½Ð° Kaggle 0.512 - 2248 Ð¼ÐµÑÑ‚Ð¾


"""ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° Ð½ÐµÑ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÐµÐ¹"""

print(confusion_matrix(data_test["target"], data_test["Response"]))
