# Курсовой проект. Машинное обучение. Классификация и ансамбли



### Описание задания

Загрузите данные, приведите их к числовым, заполните пропуски,
нормализуйте данные и оптимизируйте память.

Сформируйте параллельный ансамбль из CatBoost, градиентного бустинга,
XGBoost и LightGBM. Используйте лучшие гиперпараметры, подобранные 
ранее, или найдите их через перекрестную проверку. Итоговое решение 
рассчитайте на основании самого точного предсказания класса у 
определенной модели ансамбля: выберите для каждого класса модель, 
которая предсказывает его лучше всего.

Проведите расчеты и выгрузите результат в виде submission.csv

Данные:
* video.ittensive.com/machine-learning/prudential/train.csv.gz
* video.ittensive.com/machine-learning/prudential/test.csv.gz
* video.ittensive.com/machine-learning/prudential/sample_submission.csv.gz

Итоговый файл с кодом (.py или .ipynb) выложите в github с портфолио.



### Решение:

Импортируем необходимые модули для обработки данных Numpy, Pandas, sklearn,
catboost, xgboost, lightgbm.

Загрузим данные. Произведем предобработку, для этого используем функцию
data_preprocess. Выделим для расчетов набор столбцов. Произведем нормализацию
данных.

При обработке больших объемов данных, одной из задач является оптимизация
памяти, т.к. объем загружаемых данных может быть больше чем объем оперативной
памяти. Для оптимизации памяти необходимо переопределить тип данных для
отдельных серий. Привести некоторые данные к целым значениям, удалить
неиспользуемые серии, а так же удалить промежуточные наборы данных. Создадим
функцию оптимизации памяти reduce_mem_usage.

Для расчета результатов, построим базовые модели XGBoost, CatBoost, LightGBM,
модель Градиентного бустинга. Для этого используем методы XGBClassifier,
CatBoostClassifier, LGBMRegressor и GradientBoostingClassifier.

Произведем расчет предсказаний по рассчитанным моделям.

Сформируем и выгрузим результаты расчетов. Для этого загрузим примерный файл,
заменим в нем результаты и сохраним. Число строк в файле будет равно размену
набора данных + 1 заголовочная строка. Выгружаем результаты 
в CSV-файл "submission.csv".

Рассчитаем точность классификации на обучающих данных, используя функцию
vote_class_enumerate. Результаты проверки моделей:

Проверяем модель: xgb

Максимальная оценка: 0.9183863647385216

Проверяем модель: cb

Максимальная оценка: 0.923490383367688

Проверяем модель: gbc

Максимальная оценка: 0.9235123428913579

Проверяем модель: lgb

Максимальная оценка: 0.9238814092725737


Лучший результат: 0.924

Далее составляем матрицу неточностей:

[[    0     0     0     0     0     0     0     0     0]

 [ 6207     0     0     0     0     0     0     0     0]

 [    0  6552     0     0     0     0     0     0     0]

 [    0     0  1013     0     0     0     0     0     0]

 [    0     0     0  1428     6     0     0     0     0]

 [    0     0     0     0  5308    89    18    10     0]

 [    0     0     0     0   116 11143   201     0     0]

 [    0     0     0     0     1     0  7788   863     0]

 [    0     0     0     0     1     1    20 18616     0]]



## Технологии в решениях
Python 3.9,
Numpy,
Pandas,
sklearn.


### Студент
Валерий Калинин